\section{Concentration of Measure}

Concentration of measure is the notion that it is hard for independent random variables to work together. For instance, if we have independent random variables $X_1, \dots, X_n$ that lie in the interval $O(1)$, then
\[
    S_n = \sum X_i
\]
sharply concentrates in the interval $O(\sqrt{n})$ (which is much smaller than $O(n)$).

There are two methods used to derive bounds to control the behavior of $S_n$ (also called large deviation inequalities). They are the moment method and the exponential moment method.

\paragraph{Moment Method.} The $k$-th moment method involves bounding $\bE |S_n|^k$ and then applying Markov's inequality to bound $\bP(|S_n|^k \geq a^k) = \bP(|S_n| \geq a)$.

For even $k$, $\bE|S_n|^k = \bE S_n^k$, the $k$-th moment, which can estimated via a combinatorial argument. $\bE|S_n|^k$ is also easy to bound when $k = 1$. However, it is hard to bound $\bE|S_n|^k$ for odd $k > 1$, so this moment method does not work with odd $k > 1$.

(For odd $k > 1$ could we try bounding $\bE S_n^k$ instead? Yes, but because Markov's inequality requires nonnegative RVs, we cannot bound $\bP(S_n^k \geq a^k) $ by bounding $\bE S_n^k$.)

\paragraph{Exponential Moment Method.} The exponential moment method works by noting
\[
    \bP(S_n \geq a) = \bP( e^{tS_n} \geq e^{ta}), \quad t > 0.
\]
If we can bound $\bE e^{tS_n}$ then applying Markov's inequality (as before), we can bound $\bP( e^{tS_n} \geq e^{ta})$ and thus $\bP(S_n \geq a)$.

The value $\bE e^{tS_n}$ can be computed/bounded in different fashions. If we have joint independence, then
\[
    \bE e^{tS_n} = \prod \bE e^{tX_i}.
\]
One can then compute the $e^{tX_i}$'s (e.g. use Hoeffding's lemma) and optimize over $t > 0$. (This is essentially the proof of Chernoff's inequality).

As a side note, observe $e^{tX}$ is the moment generating function of $X$. This is because
\[
    \begin{split}
        \bE e^{tX} & = \bE \left( 1 + tX + \frac{(tX)^2}{2!} + \dots \right) \\
                   & = 1 + t\bE X + \frac{t^2}{2!}\bE X^2 + \dots
    \end{split}
\]

\paragraph{Results.} Using higher moments for the moment method produces stronger large deviation inequalities, but requires stronger boundedness and independence conditions. More precisely, the $k$-th moment method produces a $\l^{-k}$ falloff, but requires the existence of $\bE |S_n|^k$ and $k$-wise independence.

The exponential moment method likewise produces a strong inequality (a Gaussian falloff!), but requires bounded RVs (to ensure the existence of the moment generating function) and joint independence.

\paragraph{Truncation.} How do we bound $S_n$ if the strong boundedness conditions aren't met (i.e. the $X_i$'s are unbounded)? First, we split each random variable into a bounded body and an unbounded tail
\[
    X = X_{\leq N} + X_{\geq N}.
\]
Then, we define
\[
    S_{n, \leq N} = \sum X_{i, \leq N}, \quad S_{n, \geq N} = \sum X_{i, \geq N}.
\]
We then bound the two parts differently and combine the results. We apply stronger inequalities to $S_{\leq N}$ (as it has strong boundedness conditions) and weaker inequalities to $S_{\geq N}$ (which is unbounded). This divide-and-conquer technique is called truncation and can be used to partially extend strong results to situations that lack strong boundedness conditions.

\paragraph{Lipschitz.} More generally, we need not consider $S_n$ but rather any function
\[
    F = F(X_1, \dots, X_n)
\]
with certain regularity conditions (e.g. Lipschitz and convex). To derive bounds to control the behavior of $F$, we can generalize the exponential moment method to bound
\[
    \bP(F \geq a) = \bP(e^{tF} \geq e^{ta}), \quad t > 0
\]
by bounding the value $\bE e^{tF}$ and applying Markov. The exact bounds on $\bE e^{tF}$ depend on the regularity conditions of $F$ and certain tricks (like log-Solobev with Herbst's argument).

%%% Local Variables:
%%% TeX-master: "main"
%%% End:
