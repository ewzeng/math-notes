\section*{The Semicircular Law}

We would like to study the asympototic behavior of eigenvalues. As the number of eigenvalues varies with the size of a matrix, it only really makes sense to study the distribution of eigenvalues (as matrix size $\to \i$). As the natural object to represent ``distributions'' are pdfs (i.e. probability measures) and cdfs, thus for a matrix $M_n$ (of size $n \times n$), we define the associated distribution of eigenvalues (aka the ESD) to be the probability measure
\[
    \mu_{M_n} = \frac{1}{n} \sum_{j = 1}^n \d_{\l_j}.
\]

When $M_n$ is a random matrix, the associated probability measure will be a random probability measure (i.e. a probability-measure-valued RV).

\paragraph{Convergence.} Given a sequence $Y_i$ of ESDs, say $Y_i \to Y$ if the corresponding cdfs convergence pointwise. (Note: even though we often visualize ESD convergence as convergence of respective pdfs, cdf convergence is the more rigorous and correct definition).

\paragraph{Semicircular Law.} Let $M_n$ be the top left $n \times n$ minors of an infinite Wigner matrix (i.e. Hermitian random matrix, i.i.d. entries except diagonal, which must be bounded). Then almost surely
\[
    \mu_{\frac{M_n}{\sqrt{n}}} \to \mu_{\text{sc}} = \frac{1}{2\pi}(4 - |x|)_+^{1/2}dx.
\]
To prove the semicircular law, we first make a simplifying observation: as eigenvalues are stable under matrix pertubations, we can perturb the matrix a little to set all diagonal entries to 0 and only consider bounded entries (takes a little work to show, second statement requires truncation argument). From here, there are two ways to proceed: the moment method and the Stieltjes method.


%%% Local Variables:
%%% TeX-master: "main"
%%% End: