\section*{Free Probability}

\paragraph{Abstract Probability Theory.} In traditional probability theory, random variables commute (because they are $\bC$-valued or $\bR$-valued measurable functions). This is not the case of random matrices, so we would like to develop theory for non-commutative spaces. We do this by abstracting out important properties from familiar examples, as seen in the table on the next page.

\newpage
\clearpage
\begin{sidewaystable}
    \begin{tabular}{|l|l|l|l|l|l|}
      \hline
      \thead{Space}  & \thead{Elements}                 & \thead{$*$-operation                                                                                                                                                                                                                                                                                                                                          \\ (Adjoints)} & \thead{$\t: \_\_ \to \bC$ \\ $\t(X^*) = \t(X)^*$} & \thead{Spec. Meas. $\mu_X$ \\ $\t(X^k) = \int_\bC X^k d\mu_X$} & \thead{Spec. Radius $\rho$ for $X=X^*$ \\ $\Supp(\mu_X) \subset [-\rho(X), \rho(X)]$}\\
      \hline
      \hline
      Reg. Prob.     & $X: \Omega \to \bC$              & Conjugation                                                                                                                                                                                  & $\t(X) = \bE[X]$                                  & $\mu_X = $ pdf                                                 & $\rho = \|X\|_\i$                         \\
      \hline
      Matrices       & $X: \bC^{n \times n}$            & Adjoint                                                                                                                                                                                      & $\t(X) = \frac{1}{n}\Tr(X)$                       & $\mu_X = $ ESD                                                 & $\rho = |\text{Largest Eigenvalue}|$      \\
      \hline
      Rand. Matrices & $X: \Omega \to \bC^{n \times n}$ & Adjoint                                                                                                                                                                                      & $\t(X) = \bE\frac{1}{n}\Tr(X)$                    & $\mu_X = \bE$ ESD                                              & $\rho = \|\text{Largest Eigenvalue}\|_\i$ \\
      \hline
    \end{tabular}
\end{sidewaystable}
\clearpage
\newpage

Thus we define a noncommutative probability space to be a unital $*$-algebra equipped with a nice $*$-linear complex-valued map $\t$ (called trace or expectation). More specifically, we require
\begin{itemize}
    \item $\t(1) = 1$ (e.g. total measure of probability space = 1).
    \item $\t(X^*X) \ge 0$ (nonnegativity, makes $\mu_X$ a nonnegative meaure and allows us to define a semi-definite inner product via $\langle X, Y \rangle = \t(X^*Y)$).
    \item $\t(XY) = \t(YX)$ (not strictly required, but makes things easier).
\end{itemize}

From these axioms, we can actually construct $\mu_X$ and $\rho$. For self-adjoint $X$, define
\[
    \rho(X) = \lim_{k \to \i}|\t(X^{2k})|^{\frac{1}{2k}}
\]
(this limit exists by monotonicity). The construction of $\mu_X$ is trickier. The basic idea for bounded (i.e. $\rho < \i$) self-adjoint $X$ is as follows:
\begin{enumerate}
    \item Define the Stieltjes transform
    \[
        s_X(z) = \t\left( (X - z)^{-1} \right)
    \]
    and use power series to extend to $\bC\backslash [-\rho(X), \rho(X)]$.
    \item Use the Stieltjes transform and complex analytic techniques to show
    \[
        |\t(P(X))| \le \sup_{x \in [-\rho(X), \rho(X)]} |P(x)|
    \]
    for any polynomial $P: \bC \to \bC$. The key is to apply Cauchy's integral formula to the Stieltjes transform (see below) and compute the integral over certain circles in the domain.
    \item Using 2 and the Weierstrass approximation theorem, $\t$ can be extended to a (positive) continuous linear functional on $C[-\rho(X), \rho(X)]$. Then by Reisz-Markov, we get a unique (Radon) measure $\mu_X$ supported on $[-\rho(X), \rho(X)]$ such that
    \[
        \t(P(X)) = \int_\bC P(x) d\mu_X.
    \]
    [Note we said $\t$ ``can be extended...'' but did not define $\t(f(X))$ for continuous $f$ because $f(X)$ may not even exist if our space is not complete. However if our space is indeed complete, then the extension allows us to define a continuous functional calculus.]
\end{enumerate}

\paragraph{Stieltjes Transform Revisited.} Observe we have
\[
    s_X(z) = - \sum_{k = 0}^\i \frac{\t(X^k)}{z^{k+1}}
\]
so $s_X(z)$ can be thought as a generating function of the $k$-th $\t$-moments. Furthermore, using Cauchy's integral formula, we can recover the moments with
\[
    \t(X^k) = - \frac{1}{2\pi i} \int_{|z| = R} \frac{s_X(z)}{z^{-k}}dz.
\]
Therefore, it is conceviable that the pointwise convergence of $s_X(z)$ is equivalent to the convergence of all moments $\t(X^k)$, which in turn is equivalent to the convergence of the probability distributions $\mu_X$. From this perspective, we can clearly see the relationship between the moment method and the Stieltjes method in proving the semicircular law. This relationship is very similar to the Fourier method vs. moment method in CLT.

[In both CLT and the semicircular law, each $X_i$ in the sequence $X_1, X_2, \dots$ comes from a different probability space, but the above argument still applies to show the convergence of the $\mu_{X_i}$'s.]

\paragraph{Free Independence.} Defining independence on a noncommutative probability space is tricky because such spaces do not have notions of sample spaces and events. The main object of a noncomm. prob. space is the expectation $\t$, so we try to draw inspiration by rewriting the tradictional notion of independence using expectation.

In a trad. prob. space, $X$, $Y$ are independent iff $\bE[f(X)g(Y)] = \bE[f(X)]\bE[g(Y)]$ for any polynomials $f, g$. (Pf: approximate indicator functions with polynomials.) This form happens to be too strong to generalize, but we can reformulate it equivalently as
\[
    \bE[f(X)] = 0, \bE[g(Y)] = 0 \implies \bE[f(X)g(Y)] = 0.
\]
Modeling after this, in a noncomm. prob. space we say $X$, $Y$ are \textbf{freely independent} if
\[
    \begin{split}
        & \bE[f_i(X)] = 0, \bE[g_i(Y)] = 0, \forall 1 \le i \le n,\\
        & \implies \bE[f_1(X)g_1(Y)f_2(X)g_2(Y)\dots f_n(X)g_n(Y)] = 0.
    \end{split}
\]
This notion can then be further extended to $k$-wise free independence. [Note: despite being modeled after independence, free independence is much stronger than independence in trad. prob. spaces.]

\paragraph{Limits.} It is not hard to show convergence of all moments $\t(X^k)$ is equivalent to convergence of probability distributions $\mu_X$ (when the $X$'s self-adjoint and have uniformly bounded spectral radius). This motivates the following definition of limits of elements in noncomm. prob. spaces: for $X_{n,1}, \dots, X_{n,k} \in (\cA_n, \t_n)$, say $X_{n,1}, \dots X_{n,k}$ converges \textbf{in the sense of moments} to $X_{\i, 1}, \dots, X_{\i,k} \in (\cA_\i, \t_\i)$ if for any sequence $i_1, \dots, i_m \in [1,k]$, we have
\[
    \t_n(X_{n, i_1},\dots, X_{n,i_m}) \to \t_\i(X_{\i, i_1}, \dots, X_{\i,i_m})
\]
as $n \to \i$. If adding the adjoints still preserves this convergence, we say it converges in the sense of $*$-moments (which is far stronger when dealing with not self-adjoint elements).

\newpage

\section*{More on Free Probability}

\paragraph{Free Independence Revisited.} Let $X,Y$ be classically independent random matrices (i.e. entries between matrices independent) with expected traces = 0. The key observation is that inspersing the matrices largely destroys any correlation; that is,
\[
    \bE[XYXY] \text{ is small.}
\]
In the commutative prob., this would not be the case as $\bE[XYXY] = \bE[X^2Y^2] = \bE[X^2]\bE[Y^2]$. This fundamental property of random matrices is exploited by the definition of free independence, which is the reason why free independence is the appropriate ``independence'' to consider in non-comm. prob. theory. (In fact, one fundamental result in RMT is that Wigner matrices are \textbf{asymptotically freely independent}.)

Indeed, it is free independence that turns free probability into a completely different beast. Results from classical prob. cannot be extended to the non-comm. case (because of the loss of commutativity); conversely, results from non-comm. prob. spaces cannot be extended to the classical case because free independence is such a strong notion that random variables in the classical case can only satisfy free independence in trivial scenarios. Thus, by giving up commutativity we obtain a more subtle notion of independence.

\paragraph{$R$-Transform.} For freely independent $X,Y$, Voiculescu proved the existence of an $R$-transform with the property
\[
    R_{X + Y} = R_X + R_Y
\]
(Source: Srivastava). The explicit transform was found later, magically turning out to be
\[
    R_X(s) = s_X^{-1}(-s) - s^{-1}.
\]
This further solidifies the importance of the Stieltjes transform as a fundamental tool of free probability, and allows for a way to compute $\mu_{X + Y}$ from $\mu_X$ and $\mu_Y$ (called the free convolution).


%%% Local Variables:
%%% TeX-master: "main"
%%% End:  